# MedRecommendation
Проект по разработке рекоммендательной системы для лекарственных препаратов на основе экстрактивных подходов и генеративных.
С одной стороны интересно посмотреть, может ли генеративная сетка или другой метод 

Подготовка:
- [ ] Организовать репозиторий по шаблону cookiecutter https://drivendata.github.io/cookiecutter-data-science/
- [ ] Завести гуглдок для лит обзора, приложить сюда ссылку с доступом для чтения или комментирования.
- [ ] завести гуглтаблицу для сбора результатов экспериментов
- [ ] Прочитать статью про корпус https://arxiv.org/abs/2105.00059

Лит. обзор. Включить в него:
- [ ] Информацию про наш корпус, модели экстракции на основе Spert и нормализации.
- [ ] прочитать про генеративные модели вроде GPT-2, mT5, и другие примерно такого же размера. И как их дообучают на конкретные задачи. Выбрать одну или 2.
- [ ] Рекомендательные системы  на основе нейронок и прочего nlp.

Экстрактивный подход.
- [x] Взять размеченный корпус, посчитать для тестовой:
  - препарат ; рейтинг ; список возможных АДР
  - симптом ; список лучших препаратов;
- [x] Взять модели и код из репозитория https://github.com/sag111/med-demo-service_react разобраться с тем, как делать предикт. Потом взять модели, обученные на тренировочной части первых фолдов. 
- [x] Сделать предикт для тестовой части корпуса и составить таблицу по предиктам.
- [x] Придумать, как эти таблицы сравнить.
- [ ] Собрать новые таблицы не по тестовой части, а по всему корпусу. Сделать предикт по не по одному тесту, а по 5 тестам (в соответствии с фолдами на которых обучались модели).
- [ ] Конфиги моделей и готовые веса моделей можно найти здесь /s/ls4/users/naumov/installations/RelationExtraction/spert/configs/bs2_best_newH (RDRS3_11_07_2022_clean_2800_XLM-R-sag_NewH_) и здесь: /s/ls4/users/naumov/installations/RelationExtraction/spert/results_naumov/RDRS/bs2_newH/RDRS3_11_07_2022_clean_3800_full/XLM-R-sag_NewH
- [ ] Модели для нормализации все там же /s/ls4/users/romanrybka/pharm_er/Pipeline_Ner_Norm/RelationExtraction/models/Norm_models/res_norm_model_dir_fold_1/ только заменить 1 на 2,3,4,5.
- [ ] Получить доступ к репозиторию https://github.com/sag111/RelationExtraction/, разобраться со скриптом обучения сперта и нормализации. Попробовать повторить точности для 1 фолда.
- [ ] Попробовать объединить модели сперта и нормализации.

## Генеративный подход.

Цель - получить модель, спосробную извлекать различные знания из большого неразмеченного корпуса.

Глобальные задачи:
- Сформировать юз кейсы для постановки экспериментов и процедуру оценки. сейчас есть 2 эксперимента, хорошо бы больше. Возможно подключить другие корпуса.
- Попробовать разные модели.
- попробовать разные подходы к предобучению.

Октябрь
- [x] Оформить в виде скриптов и закомитить код, использованный для прошлого эксперимента  (который с 6%). Прислать мне таблицу с колонками input;prediction;correc_answer для того эксперимента.
- [x] Проанализировать большой корпус для предобучения.
- [x] Взять большой, автоматически размеченный корпус /s/ls4/groups/g0126/fake_reviews/ner_800k_jsonl.zip. Проверить, как много там про косметику (которая нас мало интересует), а не про лекарства. Смотреть на размеченные там drugname и drugclass.
- [x] Составить гистограмму - на сколько отзывов сколько препаратов приходится. Это надо, чтобы определиться, нужно ли добирать корпус.
- [ ] Попробовать подобрать промты к gigachad/chatgpt для генерации рекомендаций. Оценить их по нашим тестам, сохранить их выходы.

Ноябрь
- [ ] Попробовать дообучить модель t5 на размеченном корпусе (3800 отзывов) на unsupervised задачу (предсказание следующего токена / Или предсказание заголовка или общего впечатления по тексту отзыва). 
- [ ] Протестировать её также как и не предобученную.
Попробовать дообучить модель на semi-supervised (учесть разметку), например предсказывать список адров или симптомов по предложению с препаратов, или BNE-Pos/Worse/ADE_Neg по препарату и симптомам. То есть учитывать только полезные предложения. 

Декабрь 
- [ ] Эксперименты с gpt2 моделью такие же как с t5.

Январь 
- [ ] Сформировать 4 таблицы с выборками и разделение на фолды, и проверить: сиптом-препараты, симптом-хорошие препараты, препарат-адры, препарат-класс.
- [ ] Обучить ruT5 (опционально FRED), GPT-2 на предасказание по этим таблица (инпут одна колонка, аутпут - вторая)
- [ ] Предобучить ruT5 unsupervised/semi-supervised на корпусе 3800. Задача - предсказание замаскированных спанов. Спаны маскировать либо те, которые выделены в разметке (тогда semi-supervised) либо случайные.
- [ ] На выходе должны быть точности по 5 фолдам для каждой таблицы для моделей: ruT5, GPT-2, ruT5-pretrained

Февраль 
- [ ] Предобучение ruT5 на большом корпусе - также как и на 3800, но на 200к (отфильтрованные из 700к)
- [ ] Предобучение GPT-2 (на малом, потом на большом)
- [ ] исправление ошибок, думаем, как еще улучшить точность

Март

Апрель

Май - июнь
- [ ] Написание диплома, перепроверка экспериментов


Дальнейшие планы:
- расширение корпуса
- расширить use case 
